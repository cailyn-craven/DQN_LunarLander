{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is from a code along I did with Phil Tabor in the Udemy Course \"Modern Reinforcement Learning: Deep Q Learning in PyTorch.\" Tabor shows how to code a Naive Deep Q Learning Agent that will fail due to the deadly triad. '\n",
    "https://www.udemy.com/course/deep-q-learning-from-paper-to-code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use deep neural networks for large continous state spaces. Deep neural networks are universal function approximators. They can approximate almost any continuous function including action value function in Deep Q Learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Deep Q Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives us access to the layers \n",
    "import torch.nn as nn \n",
    "# this gives us access to activation \n",
    "# functions like relu or sigmoid \n",
    "import torch.nn.functional as F \n",
    "# this gives us access to optimizers\n",
    "import torch.optim as optim \n",
    "# the base package Torch gets imported as T\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric models** for **Supervised Learning** seem very similar to ***Q-Learning Approximation Method.*** For Approximate Q Learning, the target is the Q function, and we only focused on the linear model for Reinforcement Learning so far. Is a non-linear model possible for Reinforcement Learning? In 2015, there was a breakthrough with a model called **Deep Reinforcement Learning.** \n",
    "<br>\n",
    "### Problems: \n",
    "+ If you use a function approximator with a non-linear model, convergence is not guaranteed. Off-policy bootstrapped can be a problem.   \n",
    "+ We can use bootstrapping to calculate the Q target value. Semi-gradient descent. This can cause a problem when we train the model because we don't actually have any gradient. This problem is also caused by bootstrapping. \n",
    "+ Another problem is that samples are highly correlated. \n",
    "<br> \n",
    "People call this the deadly triad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "# this gives us access to the layers \n",
    "import torch.nn as nn \n",
    "# this gives us access to activation \n",
    "# functions like relu or sigmoid \n",
    "import torch.nn.functional as F \n",
    "# this gives us access to optimizers\n",
    "import torch.optim as optim \n",
    "# the base package Torch gets imported as T\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Linear Classifier \n",
    "Pytorch classifier tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearClassifier that derives from nn.Module\n",
    "# the reason that we derive PyTorch classes from nn.Module\n",
    "# is because that gives us access to parameters of deep neural network\n",
    "class LinearClassifier(nn.Module): \n",
    "    # takes learning rate, number of classes and input dimensions as input\n",
    "    def __init__(self, alpha, num_classes, input_dims):\n",
    "        # whenever we are deriving one class from another, \n",
    "        # we want to use the super constructor \n",
    "        \"\"\"\n",
    "        The super() function in Python makes class inheritance more manageable and extensible. The function returns a temporary object that allows reference to a parent class by the keyword super.\n",
    "        The super() function has two major use cases:\n",
    "        To avoid the usage of the super (parent) class explicitly.\n",
    "        To enable multiple inheritances.\n",
    "        https://www.educative.io/edpresso/what-is-super-in-python\n",
    "        \"\"\"\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        # declare layers of deep neural network \n",
    "        # in this classifier, we have three linear layers \n",
    "        # first layer \n",
    "        #self.fcl = nn.Linear(*input_dims, 128)\n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        # second layer\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, n_classes)\n",
    "        # Adam is a type of stochastic gradient descent with momentum\n",
    "        # it is an adaptive learning rate algorithm \n",
    "        # so it is highly useful for classifiers \n",
    "        # for the Deep Q Learning algorithm, the paper does opt for RMSprop optimizer\n",
    "        # self.parameters() comes from nn.module and ells us wha we want to optimize \n",
    "        # the learning rate is the rate at which we want to optimize that quantity \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        # for Deep Q Learning, we will want to use something like nn.MSELoss()\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss() # typical loss function for multi-class classification problem\n",
    "        # use GPU if it is available \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        # send entire network to the device \n",
    "        self.to(self.device)\n",
    "    \n",
    "    # PyTorch handles backpropagation algorithm for us,\n",
    "    # but we need to define own forward propagation algorithm\n",
    "    def forward(self, data):\n",
    "        # in our case the data is going to be screen images \n",
    "        # pass that data through first linear layer and activate it \n",
    "        # for Deep Q learning, we won't be using sigmoid function\n",
    "        layer1 = F.sigmoid(self.fcl(data))\n",
    "        # send activated output through second linear layer \n",
    "        # pass through linear layer first, then activate it \n",
    "        # order of operations from inner parentheses to outer parentheses \n",
    "        layer2 = F.sigmoid(self.fcl(layer1))\n",
    "        # don't activate output of this layer\n",
    "        # cross entropy loss will handle activation for us \n",
    "        layer3 = self.fcl(layer2)\n",
    "        \n",
    "        return layer3\n",
    "    \n",
    "    def learn(self, data, labels):\n",
    "        # need to zero out gradiens for optimizer\n",
    "        self.optimizer.zero_grad()\n",
    "        # can't pass in numpy arrays as input, \n",
    "        # need to pass in tensors \n",
    "        data = T.tensor(data).to(self.device)\n",
    "        # Pytorch also has a capital Tensor constructor\n",
    "        # the lower case version preserves the type of the source data \n",
    "        # which is exactly what we want to do\n",
    "        labels = T.tensor(labels).to(self.device)\n",
    "        \n",
    "        predictions = self.forward(data)\n",
    "        # how far the predictions are from the actual labels \n",
    "        cost = self.loss(predictions, labels)\n",
    "        # these next two functions are critical to the functionality \n",
    "        # of the learning loop\n",
    "        # back propagate that cost\n",
    "        cost.backward()\n",
    "        # step the optimizer \n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using agent class, but separate game playing loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derives from nn.Module\n",
    "# the reason that we derive PyTorch classes from nn.Module\n",
    "# is because that gives us access to parameters of deep neural network\n",
    "class LinearDeepQNetwork(nn.Module):\n",
    "    # takes learning rate, actions, and input dimensions as input\n",
    "    def __init__(self, lr, num_actions, input_dims):\n",
    "        # whenever we are deriving one class from another, \n",
    "        # we want to use the super constructor \n",
    "        \"\"\"\n",
    "        The super() function in Python makes class inheritance more manageable and extensible. The function returns a temporary object that allows reference to a parent class by the keyword super.\n",
    "        The super() function has two major use cases:\n",
    "        To avoid the usage of the super (parent) class explicitly.\n",
    "        To enable multiple inheritances.\n",
    "        https://www.educative.io/edpresso/what-is-super-in-python\n",
    "        \"\"\"\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        # declare layers of deep neural network \n",
    "        # in this classifier, we have three linear layers \n",
    "        # first layer, 128 neurons for fully connected layer \n",
    "        #self.fcl = nn.Linear(*input_dims, 128)\n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        \"\"\"\n",
    "        second layer also linear, 128 neurons as input\n",
    "        actions as output \n",
    "        second layer\n",
    "        we have n_actions as output for this layer \n",
    "        because we want to calculate estimate for Q which is a state, action pair\n",
    "        Q is a state, value action function, \n",
    "        we want to get out the value of each action for that state \n",
    "        dimensionality must correspond to number of actions for have for our environment \n",
    "        \"\"\"\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "        #self.fc3 = nn.Linear(256, n_classes)\n",
    "        \"\"\"\n",
    "        Adam is a type of stochastic gradient descent with momentum\n",
    "        it is an adaptive learning rate algorithm \n",
    "        so it is highly useful for classifiers \n",
    "        for the Deep Q Learning algorithm, the paper does opt for RMSprop optimizer\n",
    "        self.parameters() comes from nn.module and ells us wha we want to optimize \n",
    "        the learning rate is the rate at which we want to optimize that quantity \n",
    "        \"\"\"\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        # for Deep Q Learning, we will want to use something like nn.MSELoss()\n",
    "        \n",
    "        self.loss = nn.MSELoss() # typical loss function for multi-class classification problem\n",
    "        # use GPU if it is available \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        # send entire network to the device \n",
    "        self.to(self.device)\n",
    "    # PyTorch handles backpropagation algorithm for us,\n",
    "    # but we need to define own forward propagation algorithm\n",
    "    def forward(self, state):\n",
    "        # in our case the data is going to be screen images \n",
    "        # pass that data through first linear layer and activate it \n",
    "        # for Deep Q learning, we won't be using sigmoid function\n",
    "        #layer1 = F.relu(self.fcl(state))\n",
    "        layer1 = F.relu(self.fc1(state))\n",
    "        # don't activate output of this layer\n",
    "        # cross entropy loss will handle activation for us\n",
    "        actions = self.fc2(layer1)\n",
    "        return actions \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this agent class is adapted from the class notebook \n",
    "#deterministic rule\n",
    "# first choose random action\n",
    "# monitor vc and vp while keep going to the same direction\n",
    "# if the sign of the two velocities are different, flip the direction (action)\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, input_dims, num_actions, alpha, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_dec_rate=1e-5):\n",
    "    #def __init__(self, input_dims, num_actions, alpha, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_dec_rate=1e-5):\n",
    "            \n",
    "        # save arguments as member variables of class\n",
    "        self.alpha = alpha\n",
    "        self.input_dims = input_dims\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \"\"\"\n",
    "        It is important that epsilon decreases over time.\n",
    "        The agent starts out doing a higher rate of exploration.\n",
    "        As the agent learns more, the agent exploits high reward actions\n",
    "        and decreass he rate of exploration. \n",
    "        \"\"\"\n",
    "        self.epsilon_dec_rate = epsilon_dec_rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.action_space = [*range(self.num_actions)]\n",
    "        \n",
    "        \n",
    "        # an agent has a Q estimate, an agent is not a Q estimate\n",
    "        # the Q estimate is one parameter of agent \n",
    "        # for choosing actions, decreasing epsilon, and learning from its experiences\n",
    "        # Deep Q Network is in its own class, use as member variable for agent class\n",
    "        self.Q = LinearDeepQNetwork(self.alpha, self.num_actions, self.input_dims)\n",
    "        \n",
    "       \n",
    "    def select_action(self, observation): \n",
    "        # takes observation of environment as input \n",
    "        # we are using an epsilon greedy strategy to select actions\n",
    "        # random.random() returns a number between 0.0 and 1.0 \n",
    "        # https://stackoverflow.com/questions/33359740/random-number-between-0-and-1-in-python\n",
    "        # if greater than epsilon, take greedy action\n",
    "        if random.random() > self.epsilon:\n",
    "            # make sure tha observation is Pytorch tensor \n",
    "            # that is sent to actual kuda device \n",
    "            # Pytorch is quite particular about datatypes of tensors passed in \n",
    "            # device is a property of the LinearDeepQNetwork\n",
    "            # set dtype of float to be extra cautious about making sure everything matches up\n",
    "            state = T.tensor(observation, dtype=T.float).to(self.Q.device)\n",
    "            actions = self.Q.forward(state)\n",
    "            # take argmax of tensor actions \n",
    "            # need to dereference it with a .item function\n",
    "            # this is a nuance of the Pytorch framework,\n",
    "            # when you feed forward state through the Q network,\n",
    "            # you don't get back a numpy array, you get back a tensor\n",
    "            # a tensor won't serve as appropriae input to OpenAI Gym's environment\n",
    "            # the .item function gets the numpy array out of it \n",
    "            action = T.argmax(actions).item()\n",
    "        # otherwise take a random action \n",
    "        else: \n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        TODO: look at other strategies for this \n",
    "        One source on Exploration/Exploitation Trade-off: \n",
    "        https://www.manifold.ai/exploration-vs-exploitation-in-reinforcement-learning\n",
    "        References the Decaying Epsilon Greedy Algorithm \n",
    "        Review algorithm options\n",
    "        needs to go to minimum value over time,\n",
    "        that minimum value must be finite \n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            # TODO: currently this is a linear dependence\n",
    "            # you could also use a 1/square root dependence, or an exponential or logarithmic dependence\n",
    "            # isn't really critical \n",
    "            # do need to make sure it reduces epsilon to min epsilon value over time \n",
    "            self.epsilon = self.epsilon*self.epsilon_dec_rate \n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min \n",
    "    \n",
    "    def update_Qval(self, state, action, reward, successor_state):\n",
    "        # first thing: zero gradients\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        # we have numpy arrays, \n",
    "        # we need to convert them to Pytorch Kuda Tensors\n",
    "        states = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
    "        actions = T.tensor(action)\n",
    "        rewards = T.tensor(reward).to(self.Q.device)\n",
    "        successor_state = T.tensor(successor_state, dtype=T.float).to(self.Q.device)\n",
    "        \n",
    "        # do feed forward to calculate update equation\n",
    "        # we want the predicted values for the current state of the environment\n",
    "        # we want the delta between the action the agent actually took,\n",
    "        # and the maximum action it could have taken in that particular state\n",
    "        # we want to move in the direction of the maximum action for a given state\n",
    "        # distance between where we are and that target\n",
    "        # take action indices from that Q predicted tensor \n",
    "        q_pred = self.Q.forward(states)[actions]\n",
    "        # we want the maximum action for the agent's estimate of the resulting states\n",
    "        q_next = self.Q.forward(successor_state).max()\n",
    "        # reward times gamma plus maximum action in next state\n",
    "        q_target = reward + self.gamma * q_next\n",
    "        # difference between action the agent took and action agent could have taken \n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        # backpropagate and step optimizer \n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()\n",
    "        self.decay_epsilon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores, epsilons):\n",
    "    \"\"\"\n",
    "    create figure with two subplots\n",
    "    one subplot will correspond to scores agent received\n",
    "    other subplot corresponds to epsilons over time \n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, label=\"1\")\n",
    "    ax2 = fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "    \n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "    \n",
    "    N=len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        # creating numpy array to keep track of scores from the previous 100 games \n",
    "        # for games 0 - 100, use current score \n",
    "        running_avg[t] = np.mean(scores[max(0, t-100):(t+1)])\n",
    "    \n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    # set_visible to False so we don't clutter up plot\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    # set ticks to the right side \n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel('Score', color='C1')\n",
    "    # label on the right side \n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.tick_params(axis='y', colors='C1')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 23.0 avg score 23.0 epsilon 0.01\n",
      "episode  100 score 33.0 avg score 34.7 epsilon 0.01\n",
      "episode  200 score 26.0 avg score 27.6 epsilon 0.01\n",
      "episode  300 score 34.0 avg score 30.1 epsilon 0.01\n",
      "episode  400 score 23.0 avg score 27.7 epsilon 0.01\n",
      "episode  500 score 26.0 avg score 27.5 epsilon 0.01\n",
      "episode  600 score 25.0 avg score 26.5 epsilon 0.01\n",
      "episode  700 score 31.0 avg score 27.6 epsilon 0.01\n",
      "episode  800 score 31.0 avg score 31.0 epsilon 0.01\n",
      "episode  900 score 40.0 avg score 30.5 epsilon 0.01\n",
      "episode  1000 score 26.0 avg score 30.2 epsilon 0.01\n",
      "episode  1100 score 22.0 avg score 29.1 epsilon 0.01\n",
      "episode  1200 score 23.0 avg score 29.0 epsilon 0.01\n",
      "episode  1300 score 32.0 avg score 28.8 epsilon 0.01\n",
      "episode  1400 score 29.0 avg score 25.4 epsilon 0.01\n",
      "episode  1500 score 35.0 avg score 26.8 epsilon 0.01\n",
      "episode  1600 score 32.0 avg score 28.8 epsilon 0.01\n",
      "episode  1700 score 24.0 avg score 29.8 epsilon 0.01\n",
      "episode  1800 score 41.0 avg score 34.5 epsilon 0.01\n",
      "episode  1900 score 66.0 avg score 39.1 epsilon 0.01\n",
      "episode  2000 score 39.0 avg score 44.0 epsilon 0.01\n",
      "episode  2100 score 65.0 avg score 43.6 epsilon 0.01\n",
      "episode  2200 score 42.0 avg score 34.5 epsilon 0.01\n",
      "episode  2300 score 47.0 avg score 33.5 epsilon 0.01\n",
      "episode  2400 score 54.0 avg score 37.0 epsilon 0.01\n",
      "episode  2500 score 35.0 avg score 37.1 epsilon 0.01\n",
      "episode  2600 score 25.0 avg score 36.4 epsilon 0.01\n",
      "episode  2700 score 65.0 avg score 37.6 epsilon 0.01\n",
      "episode  2800 score 23.0 avg score 40.8 epsilon 0.01\n",
      "episode  2900 score 75.0 avg score 43.0 epsilon 0.01\n",
      "episode  3000 score 132.0 avg score 138.0 epsilon 0.01\n",
      "episode  3100 score 453.0 avg score 203.0 epsilon 0.01\n",
      "episode  3200 score 500.0 avg score 435.9 epsilon 0.01\n",
      "episode  3300 score 16.0 avg score 382.8 epsilon 0.01\n",
      "episode  3400 score 10.0 avg score 176.3 epsilon 0.01\n",
      "episode  3500 score 129.0 avg score 13.2 epsilon 0.01\n",
      "episode  3600 score 176.0 avg score 142.6 epsilon 0.01\n",
      "episode  3700 score 326.0 avg score 277.1 epsilon 0.01\n",
      "episode  3800 score 500.0 avg score 470.1 epsilon 0.01\n",
      "episode  3900 score 9.0 avg score 130.6 epsilon 0.01\n",
      "episode  4000 score 8.0 avg score 9.3 epsilon 0.01\n",
      "episode  4100 score 35.0 avg score 15.3 epsilon 0.01\n",
      "episode  4200 score 24.0 avg score 22.2 epsilon 0.01\n",
      "episode  4300 score 36.0 avg score 25.1 epsilon 0.01\n",
      "episode  4400 score 46.0 avg score 28.1 epsilon 0.01\n",
      "episode  4500 score 41.0 avg score 31.3 epsilon 0.01\n",
      "episode  4600 score 37.0 avg score 32.7 epsilon 0.01\n",
      "episode  4700 score 47.0 avg score 34.3 epsilon 0.01\n",
      "episode  4800 score 66.0 avg score 44.2 epsilon 0.01\n",
      "episode  4900 score 500.0 avg score 142.8 epsilon 0.01\n",
      "episode  5000 score 19.0 avg score 379.9 epsilon 0.01\n",
      "episode  5100 score 17.0 avg score 28.8 epsilon 0.01\n",
      "episode  5200 score 19.0 avg score 15.7 epsilon 0.01\n",
      "episode  5300 score 19.0 avg score 19.8 epsilon 0.01\n",
      "episode  5400 score 15.0 avg score 19.6 epsilon 0.01\n",
      "episode  5500 score 22.0 avg score 16.8 epsilon 0.01\n",
      "episode  5600 score 13.0 avg score 19.0 epsilon 0.01\n",
      "episode  5700 score 13.0 avg score 19.1 epsilon 0.01\n",
      "episode  5800 score 17.0 avg score 22.1 epsilon 0.01\n",
      "episode  5900 score 18.0 avg score 18.0 epsilon 0.01\n",
      "episode  6000 score 14.0 avg score 17.1 epsilon 0.01\n",
      "episode  6100 score 22.0 avg score 25.3 epsilon 0.01\n",
      "episode  6200 score 19.0 avg score 17.1 epsilon 0.01\n",
      "episode  6300 score 14.0 avg score 16.5 epsilon 0.01\n",
      "episode  6400 score 14.0 avg score 26.2 epsilon 0.01\n",
      "episode  6500 score 22.0 avg score 15.6 epsilon 0.01\n",
      "episode  6600 score 34.0 avg score 29.4 epsilon 0.01\n",
      "episode  6700 score 10.0 avg score 25.9 epsilon 0.01\n",
      "episode  6800 score 44.0 avg score 24.6 epsilon 0.01\n",
      "episode  6900 score 13.0 avg score 26.2 epsilon 0.01\n",
      "episode  7000 score 36.0 avg score 26.4 epsilon 0.01\n",
      "episode  7100 score 21.0 avg score 27.3 epsilon 0.01\n",
      "episode  7200 score 38.0 avg score 29.0 epsilon 0.01\n",
      "episode  7300 score 37.0 avg score 32.1 epsilon 0.01\n",
      "episode  7400 score 59.0 avg score 33.3 epsilon 0.01\n",
      "episode  7500 score 64.0 avg score 63.9 epsilon 0.01\n",
      "episode  7600 score 11.0 avg score 200.6 epsilon 0.01\n",
      "episode  7700 score 9.0 avg score 10.3 epsilon 0.01\n",
      "episode  7800 score 10.0 avg score 9.4 epsilon 0.01\n",
      "episode  7900 score 10.0 avg score 9.4 epsilon 0.01\n",
      "episode  8000 score 9.0 avg score 9.5 epsilon 0.01\n",
      "episode  8100 score 228.0 avg score 356.5 epsilon 0.01\n",
      "episode  8200 score 225.0 avg score 292.3 epsilon 0.01\n",
      "episode  8300 score 110.0 avg score 157.8 epsilon 0.01\n",
      "episode  8400 score 101.0 avg score 89.7 epsilon 0.01\n",
      "episode  8500 score 131.0 avg score 95.2 epsilon 0.01\n",
      "episode  8600 score 9.0 avg score 13.1 epsilon 0.01\n",
      "episode  8700 score 8.0 avg score 10.2 epsilon 0.01\n",
      "episode  8800 score 9.0 avg score 10.7 epsilon 0.01\n",
      "episode  8900 score 9.0 avg score 9.9 epsilon 0.01\n",
      "episode  9000 score 8.0 avg score 11.8 epsilon 0.01\n",
      "episode  9100 score 10.0 avg score 9.8 epsilon 0.01\n",
      "episode  9200 score 9.0 avg score 19.0 epsilon 0.01\n",
      "episode  9300 score 12.0 avg score 61.3 epsilon 0.01\n",
      "episode  9400 score 380.0 avg score 165.6 epsilon 0.01\n",
      "episode  9500 score 9.0 avg score 196.3 epsilon 0.01\n",
      "episode  9600 score 23.0 avg score 61.8 epsilon 0.01\n",
      "episode  9700 score 31.0 avg score 60.9 epsilon 0.01\n",
      "episode  9800 score 15.0 avg score 31.0 epsilon 0.01\n",
      "episode  9900 score 26.0 avg score 25.2 epsilon 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "n_games = 10000\n",
    "scores = []\n",
    "eps_history = []\n",
    "\n",
    "agent = Agent(alpha=0.0001, input_dims=env.observation_space.shape,\n",
    "                  num_actions=env.action_space.n)\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(obs)\n",
    "        obs_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.update_Qval(obs, action, reward, obs_)\n",
    "        obs = obs_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print('episode ', i, 'score %.1f avg score %.1f epsilon %.2f' %\n",
    "                (score, avg_score, agent.epsilon))\n",
    "# plot out the learning curve \n",
    "# index games on the x axis from 1 \n",
    "# makes more sense to play first game than 0th game\n",
    "x = [i+1 for i in range(n_games)]\n",
    "plot_learning_curve(x, scores, eps_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon decreases over time as you would expect until it reaches its first peak. Then something mysterious happens. After reaching the peak, it catastrophically drops back down to its first minimum. This occurs several times. Our naive implementation of Deep Q Learning Agent as applied to a continuous state space simply does not work. \n",
    "Problems with this approach:\n",
    "+ Agent only learns from one example: the agent takes thousands of steps and this experience is effectively discarded each time \n",
    "+ Enormous parameter space: Neural network is a function approximator \n",
    "+ Large epsilon enables exploration: every time we start an episode, we start from a random point in that enormous parameter space. We jump around that space quite easily when the epsilon is large. Max actions tend to favor parts of the space that look familiar. As we gradually reduce epsilon, jumps become smaller. This makes getting caught in a local minimum more likely. This explains why our score drops off when epsilon goes to 0.01. \n",
    "+ We are using one network for two tasks: the same network is used to evaluate the maximum action and to choose that maximum action. This gets updated at every step so we are really chasing a moving target. \n",
    "+ That max has potential to **bias** our agent. We are always evaluating with respect to the max so this is the very definition of bias. \n",
    "+ Could it learn with more games? Perhaps. But it might be very costly. Cartpole is just a vector of four elements. Games like Atari have far more dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Implementation didn't work out. Took input vector from the environment as input. Passed it through linear layers as input. Used simple state representations of our environment. It won't always be possible to do this. We are going to switch from observation vectors to more complex observations of the environment. To use screen images, we are going to need to use **convolutions**. A convolution is a matrix product that **performs feature extraction.** The features extracted tend to be unrecognizable to us as humans. \n",
    "<br> A convolution is formed by sliding a smaller matrix, a **filter**, over a larger matrix, the **input image** and taking the matrix product each time. The size of the filter is called the **kernel.** The **stride** is how much space it skips each time it slides. The resulting numbers are then stored as a new matrix that is smaller than the original matrix. In reality, we will use a number of filters. For example, if we use 32 filters, the 32 filters will be rastered over the image resulting in 32 smaller matrices. The output of the convolutions are **activated**, typically wih the **relu function.** The smaller activated matrices serve as input for regular linear layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning \n",
    "DeepQ Network was developed by DeepMind in 2015. The function approximator is a Deep Convolutional Neural Network. A Deep Convolutional Neural Network is highly non-linear so it works well but isn't guaranteed to converge. This algorithm required some clever tricks to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Samples are correlated \n",
    "They solved this by using **replay buffers.** Randomly sample in different order. Then use gradient descent. Then samples are no longer correlated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Moving Target Q Value so Regression Target is not Stable \n",
    "Separate target network from prediction network. \n",
    "\n",
    "Note: the information in this notebook is from Geena Kim's Fall 2012 Deep Q Learning Lecturs for the University of Colorado 3202 AI Course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
